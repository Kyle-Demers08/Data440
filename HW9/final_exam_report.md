# Kyle Demers
# Final exam - Email Classification
Due: Tuesday, December 20, 2022 by 11:59pm Read through the entire assignment before starting. Do not wait until the last minute to start working on it.

## Assignment
The goal of this assignment is to classify emails into two groups based on topic -- either relevant (on topic) or non-relevant (off topic). You may choose the topic based on what types of emails you typically receive (or what you have access to).

Important: Much of the code for this assignment is provided for you, therefore, your report must include a high-level description of how the code works and answers to all of the sub-questions asked.

Name your report for this assignment final_exam_report with the proper file extension.

### Q1 - Create two datasets, Testing and Training (2 points)
Create two datasets, Training and Testing. You may choose a topic to classify your emails on (but choose only 1 topic). This can be spam, shopping emails, school emails, etc.

The Training dataset should consist of:

20 text documents for email messages you consider relevant to your chosen topic

20 text documents for email messages you consider non-relevant to your chosen topic

The Testing dataset should consist of:

5 text documents for email messages you consider relevant to your chosen topic

5 text documents for email messages you consider non-relevant to your chosen topic

Make sure that these are plain-text documents and that they do not include HTML tags. The documents in the Testing set should be different from the documents in the Training set.

Upload your datasets to your GitHub repo. Please do not include emails that contain sensitive information.

[training deals](https://github.com/Kyle-Demers08/Data440/tree/main/HW9/Deals)

[training non deals](https://github.com/Kyle-Demers08/Data440/tree/main/HW9/non_deals)

[testing set](https://github.com/Kyle-Demers08/Data440/tree/main/HW9/testingset)

### A: What topic did you decide to classify on?

I decided to classify emails based on whether or not the email was providing deals. Since we are around the holidays, I always want to look for deals to buy christmas presents. I find it a bit tedious to dig through my archrives to find a fun item at a good price. With further implementation, this would be useful to auto assign it to a folder in my inbox.

### Q2 - Naive Bayes classifier (3 points)
Use the example code in the class Colab notebook to train and test the Naive Bayes classifier.

Use your Training dataset to train the Naive Bayes classifier.
Use your Testing dataset to test the Naive Bayes classifier.
Create a table to report the classification results for each email message in the Testing dataset. The table should include what the classifier reported (relevant or non-relevant) and the actual classification.

### Answer:

Before training my model I had to make a few adjustments to the getwords script in order to read in txt files.

```python
def getwords(doc):
    splitter = re.compile('\W+')  # different than book
    with open(doc) as f:
        lines = f.readlines()
    words = []
    for line in lines: 
        for s in splitter.split(line):
            if len(s) >2 and len(s)<20:
                words.append(s.lower())
    return words
```

I then created a function to read in all the files from a folder and use the naive bayes classifier

```python
def traindata(folder,label = 'deal'):
    '''
    Trains the data on every txt file in a folder
    folder: where the emails are stored
    label: if the email is a deal or not
    '''
    owd = os.getcwd() #get current working directory
    os.chdir(folder) #change working directory to the file path
    for file in os.listdir(): #for each email
        if file.endswith('.txt'): #makesure its a txt file
            cl.train(file,label) #train the data
    os.chdir(owd) #reset working directory when done
    return("Total items: " + str(cl.totalcount()),"Categories:", cl.categories(),'number of ' + label + ' : ' + str(cl.catcount(label)))
```

I didn't create a table, but rather an output to make sure I was getting the correct classification. For this I had to change my working directory to my testing files and then run the following code.

<img width="271" alt="image" src="https://user-images.githubusercontent.com/112887807/208540794-65c25c14-e68e-4ef1-8fab-ea1fabc9fc27.png">

The outputs ensured that I got the correct answer. 

### A: For those emails that the classifier got wrong, what factors might have caused the classifier to be incorrect? You will need to look at the text of the email to determine this.

My classifier didn't get any classifications incorrect. Some words that might be important are "off" and "save" and "deal".

### Q3 - Confusion Matrix (3 points)
Draw a confusion matrix for your classification results (see Module 13, slides 42).

This should be a table in Markdown or LaTeX, NOT a screenshot of output or image generated by another program. There's an example of a LaTeX confusion matrix in the Overleaf report template.

| matrix | Predicted Deal | Predicted Not Deal|
|--------|----------------|-------------------|
|Actual Deal| 5 | 0 |
|Actual Not Deal| 0 | 5|

### A: Based on the results in the confusion matrix, how well did the classifier perform?

The classifier performed very well as it correctly identified all of the emails. 

### B: Would you prefer an email classifier to have more false positives or more false negatives? Why?

For my specific usage discussed earlier, I would rather a false positive. That means I would rather have the classifier predict it is a deal when it actually isn't as opposed to my classifier missing a deal. This is because I don't want to miss any deals. I would rather an email that doesn't have a deal be misclassified because that consequence is a mild incovenience which is a lot better than missing "the perfect gift". 

Extra Credit
### Q4: Report the precision and recall scores of your classification results. Include the formulas you used to compute these values.

Precision = 5/5 = 1

recall = 5/5 = 1

Since this doesn't demonstrate knowledge precision is the top left divided by sum of the left side in my confusion matrix while recall is top left divided by sum of the top row

Q5 (2 points)
Tune your classifier by updating weights to obtain better classification results. You may want to change the default weights (weight, ap) given to weightedprob() or the threshold used for the Bayesian classifier or change how the words are extracted from the document (for this you will need to re-train the model). Report the changes you made, re-run your Testing dataset, and show that the performance improved (either by using the confusion matrix or by computing precision and recall).

If your classifier got all of the items correct in Q2, change the weights to make the classifier perform worse and discuss the results.

Q6 (3 points)
Implement the classifier with the Multinomial model instead of the multiple Bernoulli model and re-run Q2 and Q3. Did the classification improve? Ensure to remove the unique word filter from the extractor.

For credit on this part, you must describe what you have done and discuss the differences between the Multinomial model and the multiple Bernoulli model.

